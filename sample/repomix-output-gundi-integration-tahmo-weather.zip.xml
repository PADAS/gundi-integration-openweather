This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where security check has been disabled.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: **/*.git
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Security check has been disabled - content may contain sensitive information
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
gundi-integration-tahmo-weather/
  .github/
    workflows/
      main.yaml
  app/
    actions/
      tests/
        test_client.py
        test_handlers.py
      __init__.py
      client.py
      configurations.py
      core.py
      handlers.py
    routers/
      actions.py
    services/
      tests/
        test_action_runner.py
        test_activity_logger.py
        test_gundi_api.py
        test_state_manager.py
      action_runner.py
      activity_logger.py
      errors.py
      gundi.py
      state.py
      utils.py
    settings/
      __init__.py
      base.py
      integration.py
    api_schemas.py
    conftest.py
    main.py
  docker/
    docker-compose.yml
    Dockerfile
  .gitignore
  LICENSE
  README.md
  requirements-base.in
  requirements-dev.in
  requirements.in
  requirements.txt
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="gundi-integration-tahmo-weather/.github/workflows/main.yaml">
name: Build integration
on:
  push:
    branches:
      - main
      - 'release-**'

jobs:
  vars:
    runs-on: ubuntu-latest
    outputs:
      tag: ${{ steps.vars.outputs.tag }}
      repository: ${{ steps.vars.outputs.repository }}
      tf_file_dev: ${{ steps.vars.outputs.tf_file_dev }}
      tf_file_stage: ${{ steps.vars.outputs.tf_file_stage }}
      tf_file_prod: ${{ steps.vars.outputs.tf_file_prod }}
    steps:
      - uses: actions/checkout@v4
      - id: vars
        run: |
          echo "tag=${{ github.head_ref || github.ref_name }}-$(git rev-parse --short HEAD)" >> $GITHUB_OUTPUT
          echo "repository=us-central1-docker.pkg.dev/cdip-78ca/gundi-integrations/${{ github.event.repository.name }}" >> $GITHUB_OUTPUT
          echo "tf_file_dev=environments/dev/integrations/${{ github.event.repository.name }}/terragrunt.hcl" >> $GITHUB_OUTPUT
          echo "tf_file_stage=environments/stage/integrations/${{ github.event.repository.name }}/terragrunt.hcl" >> $GITHUB_OUTPUT
          echo "tf_file_prod=environments/prod/integrations/${{ github.event.repository.name }}/terragrunt.hcl" >> $GITHUB_OUTPUT
  
  build:
    uses: PADAS/gundi-workflows/.github/workflows/build_docker.yml@v2
    needs: vars
    with:
      repository: ${{ needs.vars.outputs.repository }}
      tag: ${{ needs.vars.outputs.tag }}
      workload_identity_provider: ${{ vars.GUNDI_INTEGRATIONS_WORKLOAD_IDENTITY_PROVIDER}}
      service_account: ${{ vars.GUNDI_INTEGRATIONS_SERVICE_ACCOUNT }}

  deploy_dev:
    uses: PADAS/gundi-workflows/.github/workflows/update_hcl.yml@v2
    if: startsWith(github.ref, 'refs/heads/main')
    needs: [vars, build]
    with:
      git_repository: PADAS/gundi-integrations-v2-infra
      file_location: ${{ needs.vars.outputs.tf_file_dev }}
      key_name: image
      new_value: "${{ needs.vars.outputs.repository }}:${{ needs.vars.outputs.tag }}"
      environment: dev
    secrets:
      ssh-key: ${{ secrets.GUNDI_INTEGRATIONS_DEPLOY_KEY }}

  deploy_stage:
    uses: PADAS/gundi-workflows/.github/workflows/update_hcl.yml@v2
    if: startsWith(github.ref, 'refs/heads/release')
    needs: [vars, build]
    with:
      git_repository: PADAS/gundi-integrations-v2-infra
      file_location: ${{ needs.vars.outputs.tf_file_stage }}
      key_name: image
      new_value: "${{ needs.vars.outputs.repository }}:${{ needs.vars.outputs.tag }}"
      environment: stage
    secrets:
      ssh-key: ${{ secrets.GUNDI_INTEGRATIONS_DEPLOY_KEY }}

  deploy_prod:
    uses: PADAS/gundi-workflows/.github/workflows/update_hcl.yml@v2
    if: startsWith(github.ref, 'refs/heads/release')
    needs: [vars, build, deploy_stage]
    with:
      git_repository: PADAS/gundi-integrations-v2-infra
      file_location: ${{ needs.vars.outputs.tf_file_prod }}
      key_name: image
      new_value: "${{ needs.vars.outputs.repository }}:${{ needs.vars.outputs.tag }}"
      environment: prod
    secrets:
      ssh-key: ${{ secrets.GUNDI_INTEGRATIONS_DEPLOY_KEY }}
</file>

<file path="gundi-integration-tahmo-weather/app/actions/tests/test_client.py">
import pytest
import httpx

from pydantic import ValidationError
from app.actions.client import get_measurements_endpoint_response

@pytest.mark.asyncio
async def test_get_measurements_endpoint_response_success(mocker):
    base_url = 'https://test-api.com'
    username = 'test_user'
    password = 'test_pass'
    integration_id = 'test_integration'

    mock_stations_response = {
        "data": [
            {"id": 1, "code": "station_1", "location": {"latitude": 0.0, "longitude": 0.0}}
        ]
    }
    mock_variables_response = {
        "data": [
            {"variable": {"id": 1, "description": "Temperature", "shortcode": "temp", "units": "C"}}
        ]
    }
    mock_measurements_response = {
        "results": [
            {"statement_id": 0, "series": [{"name": "measurements", "columns": ["time", "value"], "values": [[1627849200, 25.0]]}]}
        ]
    }

    async def mock_get(url, *args, **kwargs):
        request = httpx.Request('GET', url)
        if "stations" in url:
            response = httpx.Response(200, json=mock_stations_response)
        elif "variables" in url:
            response = httpx.Response(200, json=mock_variables_response)
        elif "measurements" in url:
            response = httpx.Response(200, json=mock_measurements_response)
        else:
            response = httpx.Response(404)
        response._request = request
        return response

    mocker.patch('httpx.AsyncClient.get', side_effect=mock_get)

    readings, stations, variables = await get_measurements_endpoint_response(
        base_url=base_url,
        username=username,
        password=password,
        integration_id=integration_id
    )

    assert len(readings) == 1
    assert len(stations.stations) == 1
    assert len(variables.variables) == 1

@pytest.mark.asyncio
async def test_get_measurements_endpoint_response_unauthorized(mocker):
    base_url = 'https://test-api.com'
    username = 'test_user'
    password = 'test_pass'
    integration_id = 'test_integration'

    mocker.patch('httpx.AsyncClient.get', side_effect=httpx.HTTPStatusError(
        "Unauthorized",
        request=mocker.Mock(),
        response=mocker.Mock(status_code=401)
    ))

    with pytest.raises(httpx.HTTPStatusError):
        await get_measurements_endpoint_response(
            base_url=base_url,
            username=username,
            password=password,
            integration_id=integration_id
        )

@pytest.mark.asyncio
async def test_get_measurements_endpoint_response_http_error(mocker):
    base_url = 'https://test-api.com'
    username = 'test_user'
    password = 'test_pass'
    integration_id = 'test_integration'

    mocker.patch('httpx.AsyncClient.get', side_effect=httpx.HTTPStatusError(
        "Server Error",
        request=mocker.Mock(),
        response=mocker.Mock(status_code=500)
    ))

    with pytest.raises(httpx.HTTPStatusError):
        await get_measurements_endpoint_response(
            base_url=base_url,
            username=username,
            password=password,
            integration_id=integration_id
        )

@pytest.mark.asyncio
async def test_get_measurements_endpoint_response_invalid_format(mocker):
    base_url = 'https://test-api.com'
    username = 'test_user'
    password = 'test_pass'
    integration_id = 'test_integration'

    mock_stations_response = {
        "data": [
            {"id": 1, "code": "station_1", "location": {"latitude": 0.0, "longitude": 0.0}}
        ]
    }
    mock_variables_response = {
        "data": [
            {"id": 1, "description": "Temperature", "shortcode": "temp", "units": "C"}
        ]
    }
    # Invalid measurements response format
    mock_measurements_response = {
        "invalid_key": "invalid_value"
    }

    async def mock_get(url, *args, **kwargs):
        request = httpx.Request('GET', url)
        if "stations" in url:
            response = httpx.Response(200, json=mock_stations_response)
        elif "variables" in url:
            response = httpx.Response(200, json=mock_variables_response)
        elif "measurements" in url:
            response = httpx.Response(200, json=mock_measurements_response)
        else:
            response = httpx.Response(404)
        response._request = request
        return response

    mocker.patch('httpx.AsyncClient.get', side_effect=mock_get)

    with pytest.raises(ValidationError):
        await get_measurements_endpoint_response(
            base_url=base_url,
            username=username,
            password=password,
            integration_id=integration_id
        )
</file>

<file path="gundi-integration-tahmo-weather/app/actions/tests/test_handlers.py">
import pytest
import httpx
import respx
from httpx import Response
from unittest.mock import AsyncMock, patch
from app.actions.handlers import action_auth, action_pull_observations
from app.actions.configurations import AuthenticateConfig, PullObservationsConfig


@pytest.mark.asyncio
@respx.mock
async def test_action_auth_success(mocker):
    integration = mocker.MagicMock()
    integration.base_url = 'https://test-api.com'
    action_config = AuthenticateConfig(username='test_user', password='test_pass')

    url = f"{integration.base_url}/assets/v2/stations"
    respx.get(url).mock(return_value=Response(200, json={"detail": "Success"}))

    result = await action_auth(integration, action_config)
    assert result == {"valid_credentials": True}

@pytest.mark.asyncio
@respx.mock
async def test_action_auth_unauthorized(mocker):
    integration = mocker.MagicMock()
    integration.base_url = 'https://test-api.com'
    action_config = AuthenticateConfig(username='test_user', password='test_pass')

    url = f"{integration.base_url}/assets/v2/stations"
    respx.get(url).mock(return_value=Response(401, json={"detail": "Unauthorized"}))

    result = await action_auth(integration, action_config)
    assert result == {"valid_credentials": False}

@pytest.mark.asyncio
@respx.mock
async def test_action_auth_http_error(mocker):
    integration = mocker.Mock()
    integration.base_url = 'https://test-api.com'
    action_config = AuthenticateConfig(username='test_user', password='test_pass')

    url = f"{integration.base_url}/assets/v2/stations"
    respx.get(url).mock(return_value=Response(500, json={"detail": "Server Error"}))

    with pytest.raises(httpx.HTTPError):
        await action_auth(integration, action_config)

@pytest.mark.asyncio
async def test_action_pull_observations_success(mocker):
    integration = mocker.Mock()
    integration.base_url = 'https://test-api.com'
    action_config = PullObservationsConfig()

    mocker.patch('app.actions.client.get_auth_config', return_value=AuthenticateConfig(username='test_user', password='test_pass'))
    mocker.patch('app.actions.client.get_measurements_endpoint_response', return_value=({}, [], []))
    mocker.patch('app.services.state.IntegrationStateManager.get_state', return_value=None)
    mocker.patch('app.services.gundi.send_observations_to_gundi', return_value={})
    mocker.patch("app.services.activity_logger.publish_event", new=AsyncMock())

    result = await action_pull_observations(integration, action_config)
    assert result == {
        "observations_extracted": 0,
        "details": [],
        "message": f"No observations extracted for integration_id: {str(integration.id)}."
    }

@pytest.mark.asyncio
async def test_action_pull_observations_http_error(mocker):
    integration = mocker.Mock()
    integration.base_url = 'https://test-api.com'
    action_config = PullObservationsConfig()

    mocker.patch('app.actions.client.get_auth_config', return_value=AuthenticateConfig(username='test_user', password='test_pass'))
    mocker.patch('app.actions.client.get_measurements_endpoint_response', side_effect=httpx.HTTPError("Error"))
    mocker.patch("app.services.activity_logger.publish_event", new=AsyncMock())

    with pytest.raises(httpx.HTTPError):
        await action_pull_observations(integration, action_config)
</file>

<file path="gundi-integration-tahmo-weather/app/actions/__init__.py">
from .core import *


def setup_action_handlers():
    return discover_actions(module_name="app.actions.handlers", prefix="action_")


action_handlers = setup_action_handlers()
</file>

<file path="gundi-integration-tahmo-weather/app/actions/client.py">
import logging
import pydantic
import httpx
import re

from app.actions.configurations import AuthenticateConfig, PullObservationsConfig
from typing import List, Any
from datetime import datetime, timedelta, timezone

from app.services.errors import ConfigurationNotFound
from app.services.utils import find_config_for_action
from app.services.state import IntegrationStateManager
from app.services.activity_logger import log_action_activity
from gundi_core.schemas.v2 import LogLevel


logger = logging.getLogger(__name__)
state_manager = IntegrationStateManager()


# Pydantic Models
class Series(pydantic.BaseModel):
    name: str
    columns: List[Any] = pydantic.Field(default_factory=list)
    values: List[Any] = pydantic.Field(default_factory=list)


class Result(pydantic.BaseModel):
    statement_id: int
    series: List[Series] = pydantic.Field(default_factory=list)


class ResponseModel(pydantic.BaseModel):
    results: List[Result] = pydantic.Field(default_factory=list)


class Location(pydantic.BaseModel):
    latitude: float
    longitude: float


class StationData(pydantic.BaseModel):
    id: int
    code: str
    location: Location


class Stations(pydantic.BaseModel):
    stations: List[StationData]


class VariableData(pydantic.BaseModel):
    id: int
    description: str
    shortcode: str
    units: str

    @pydantic.validator('description')
    def sanitize_description(val):
        string = re.sub(r'(?<=[a-z])(?=[A-Z])|[^a-zA-Z]', ' ', val).strip().replace(' ', '_')
        return ''.join(string.lower())


class Variable(pydantic.BaseModel):
    variable: VariableData


class Variables(pydantic.BaseModel):
    variables: List[Variable]


class PullObservationsBadConfigException(Exception):
    def __init__(self, message: str, status_code=422):
        self.status_code = status_code
        self.message = message
        super().__init__(f'{self.status_code}: {self.message}')


class TahmoWeatherUnauthorizedException(Exception):
    def __init__(self, message: str, status_code=401):
        self.status_code = status_code
        self.message = message
        super().__init__(f'{self.status_code}: {self.message}')


def get_auth_config(integration):
    # Look for the login credentials, needed for any action
    auth_config = find_config_for_action(
        configurations=integration.configurations,
        action_id="auth"
    )
    if not auth_config:
        raise ConfigurationNotFound(
            f"Authentication settings for integration {str(integration.id)} "
            f"are missing. Please fix the integration setup in the portal."
        )
    return AuthenticateConfig.parse_obj(auth_config.data)


def get_pull_observations_config(integration):
    # Look for the login credentials, needed for any action
    config = find_config_for_action(
        configurations=integration.configurations,
        action_id="pull_observations"
    )
    if not config:
        raise ConfigurationNotFound(
            f"PullObservations settings for integration {str(integration.id)} "
            f"are missing. Please fix the integration setup in the portal."
        )
    return PullObservationsConfig.parse_obj(config.data)


async def get_measurements_endpoint_response(*, base_url:str, username:str, password:str, integration_id:str):
    readings_per_station = {}
    try:

        stations_url = f"{base_url}/assets/v2/stations"
        variables_url = f"{base_url}/assets/v2/variables"

        # Get stations info
        async with httpx.AsyncClient(timeout=120) as session:
            response = await session.get(
                stations_url,
                auth=(username, password)
            )
            response.raise_for_status()
            stations_response = response.json()
            stations = Stations.parse_obj({"stations": stations_response.get("data", [])})

        logger.info(f"Found {len(stations.stations)} stations for integration {integration_id}")

        # Get variables info
        async with httpx.AsyncClient(timeout=120) as session:
            response = await session.get(
                variables_url,
                auth=(username, password)
            )
            response.raise_for_status()
            variables_response = response.json()
            variables = Variables.parse_obj({"variables": variables_response.get("data", [])})

        logger.info(f"Found {len(variables.variables)} variables for integration {integration_id}")

        for station in stations.stations:
            # Get current state for the device
            if current_state := await state_manager.get_state(
                integration_id,
                "pull_observations",
                station.code
            ):

                latest_device_timestamp = datetime.strptime(
                    current_state.get("latest_device_timestamp"),
                    '%Y-%m-%d %H:%M:%S'
                ).replace(tzinfo=timezone.utc)

            else:
                latest_device_timestamp = (
                        datetime.now(timezone.utc) - timedelta(days=1)
                )
                
            # Hard-limit on the window of results.
            latest_device_timestamp = max(latest_device_timestamp, datetime.now(tz=timezone.utc)-timedelta(days=3))
        
            params = {"start": latest_device_timestamp.strftime('%Y-%m-%dT%H:%M:%SZ')}
            url = f"{base_url}/measurements/v2/stations/{station.code}/measurements/raw"

            logger.info(f"Fetching measurements for station {station.code} from {latest_device_timestamp} to now.")

            async with httpx.AsyncClient(timeout=120) as session:
                response = await session.get(
                    url,
                    params=params,
                    auth=(username, password)
                )
                response.raise_for_status()

            response = response.json()

            parsed_response = ResponseModel.parse_obj(response)
            readings = []

            # Check if there are valid results and assign readings
            for result in parsed_response.results:
                if result.series:
                    readings = result.series[0].values
                    logger.info(f"Found {len(readings)} readings for station {station.code}")
                    readings_per_station[station.code] = readings
                    break
                else:
                    message = f"No readings found for station {station.code}. TAHMO response: {response}"
                    logger.warning(message)
                    await log_action_activity(
                        integration_id=integration_id,
                        action_id="pull_observations",
                        title=message,
                        level=LogLevel.WARNING
                    )

    except pydantic.ValidationError as ve:
        message = f'Error while parsing TAHMO Weather Measurements endpoint. {ve.json()}'
        logger.exception(
            message,
            extra={
                "integration_id": integration_id,
                "attention_needed": True
            }
        )
        raise ve

    except Exception as e:
        message = f"Unhandled exception occurred. Exception: {e}"
        logger.exception(
            message,
            extra={
                "integration_id": integration_id,
                "attention_needed": True
            }
        )
        raise e

    return readings_per_station, stations, variables
</file>

<file path="gundi-integration-tahmo-weather/app/actions/configurations.py">
import pydantic

from .core import AuthActionConfiguration, PullActionConfiguration, ExecutableActionMixin
from app.services.utils import GlobalUISchemaOptions


class AuthenticateConfig(AuthActionConfiguration, ExecutableActionMixin):
    username: str
    password: pydantic.SecretStr = pydantic.Field(..., format="password")

    ui_global_options: GlobalUISchemaOptions = GlobalUISchemaOptions(
        order=[
            "username",
            "password",
        ],
    )


class PullObservationsConfig(PullActionConfiguration):
    pass
</file>

<file path="gundi-integration-tahmo-weather/app/actions/core.py">
import importlib
import inspect
from pydantic import BaseModel
from app.services.utils import UISchemaModelMixin


class ActionConfiguration(UISchemaModelMixin, BaseModel):
    pass


class PullActionConfiguration(ActionConfiguration):
    pass


class ExecutableActionMixin:
    pass


class PushActionConfiguration(ActionConfiguration):
    pass


class AuthActionConfiguration(ActionConfiguration):
    pass


class GenericActionConfiguration(ActionConfiguration):
    pass


def discover_actions(module_name, prefix):
    action_handlers = {}

    # Import the module using importlib
    module = importlib.import_module(module_name)
    all_members = inspect.getmembers(module)

    # Iterate through the members and filter functions by prefix
    for name, func in all_members:
        if name.startswith(prefix) and inspect.isfunction(func):
            key = name[len(prefix):]  # Remove prefix
            if (config_annotation := inspect.signature(func).parameters.get("action_config").annotation) != inspect._empty:
                config_model = config_annotation
            else:
                config_model = GenericActionConfiguration
            action_handlers[key] = (func, config_model)

    return action_handlers


def get_actions():
    return list(discover_actions(module_name="app.actions.handlers", prefix="action_").keys())
</file>

<file path="gundi-integration-tahmo-weather/app/actions/handlers.py">
import datetime
import httpx
import logging
import stamina

import app.actions.client as client
import app.services.gundi as gundi_tools

from app.services.activity_logger import activity_logger
from app.services.state import IntegrationStateManager
from app.actions.configurations import AuthenticateConfig, PullObservationsConfig


logger = logging.getLogger(__name__)


state_manager = IntegrationStateManager()


async def filter_and_transform(station, readings, stations_variables_data, integration_id, action_id):
    def transform():
        stations_info = stations_variables_data["stations"]
        variables_info = stations_variables_data["variables"]

        readings_datetime = sorted(set([a[0] for a in readings]))

        transformed_data = []
        variables_without_info = []

        for dt in readings_datetime:
            extracted_readings = [r for r in readings if r[0] == dt]

            reading_datetime = datetime.datetime.strptime(
                extracted_readings[0][0],
                "%Y-%m-%dT%H:%M:%SZ"
            )

            station_info = [s for s in stations_info.stations if s.code == extracted_readings[0][3]]

            if not station_info:
                logger.error(f"No station info available for station code '{station}'...")
                return []

            readings_dict = {}

            for reading in extracted_readings:
                code = reading[5]
                variable_info = [var for var in variables_info.variables if var.variable.shortcode == code]

                if not variable_info:
                    if code not in variables_without_info:
                        logger.warning(
                            f"No variable info available for code: '{code}', station '{station}'"
                        )
                        variables_without_info.append(code)
                    continue

                readings_dict.update(
                    {
                        f'{variable_info[0].variable.description}': reading[4]
                    }
                )

            transformed_data.append(
                {
                    "source": station,
                    "source_name": station,
                    "type": "stationary-object",
                    "subtype": "weather_station",
                    "recorded_at": reading_datetime,
                    "location": {"lat": station_info[0].location.latitude, "lon": station_info[0].location.longitude},
                    "additional": readings_dict
                }
            )

        return transformed_data

    # Get current state for the device
    current_state = await state_manager.get_state(
        integration_id,
        action_id,
        station
    )

    if current_state:
        # Compare current state with new data
        latest_device_timestamp = datetime.datetime.strptime(
            current_state.get("latest_device_timestamp"),
            '%Y-%m-%d %H:%M:%S'
        )

        last_reading_time = datetime.datetime.strptime(
            readings[-1][0],
            '%Y-%m-%dT%H:%M:%SZ'
        )

        if last_reading_time < latest_device_timestamp:
            # Data is not new, not transform
            logger.info(
                f"Excluding device ID '{station}' readings from '{last_reading_time}'"
            )
            return []

    return transform()


async def action_auth(integration, action_config: AuthenticateConfig):
    logger.info(f"Executing auth action with integration {integration} and action_config {action_config}...")
    try:
        base_url = integration.base_url or 'https://datahub.tahmo.org/services'
        url = f"{base_url}/assets/v2/stations"

        # Get stations info
        async with httpx.AsyncClient(timeout=120) as session:
            response = await session.get(
                url,
                auth=(action_config.username, action_config.password.get_secret_value())
            )
            response.raise_for_status()
    except httpx.HTTPStatusError as e:
        if e.response.status_code == 401:
            return {"valid_credentials": False}
        else:
            logger.error(f"Error authentication to TAHMO DataHub API. status code: {e.response.status_code}")
            raise e
    except httpx.HTTPError as e:
        message = f"auth action returned error."
        logger.exception(message, extra={
            "integration_id": str(integration.id),
            "attention_needed": True
        })
        raise e
    else:
        logger.info(f"Authenticated with success.")
        return {"valid_credentials": True}


@activity_logger()
async def action_pull_observations(integration, action_config):
    logger.info(f"Executing pull_observations action with integration {integration} and action_config {action_config}...")
    try:
        result = {"observations_extracted": 0, "details": {}}
        response_per_device = []
        base_url = integration.base_url or 'https://datahub.tahmo.org/services'
        async for attempt in stamina.retry_context(
                on=httpx.HTTPError,
                attempts=3,
                wait_initial=datetime.timedelta(seconds=60),
                wait_max=datetime.timedelta(seconds=60),
        ):
            auth_config=client.get_auth_config(integration)
            with attempt:
                readings, stations, variables = await client.get_measurements_endpoint_response(
                    base_url=base_url,
                    username=auth_config.username,
                    password=auth_config.password.get_secret_value(),
                    integration_id=str(integration.id)  
                )

        total_observations = 0
        if readings:
            logger.info(f"Readings pulled with success.")

            stations_variables_data = {
                "stations": stations,
                "variables": variables
            }

            for device, device_readings in readings.items():
                transformed_data = await filter_and_transform(
                    device,
                    device_readings,
                    stations_variables_data,
                    str(integration.id),
                    "pull_observations"
                )

                if transformed_data:
                    # Send transformed data to Sensors API V2
                    async for attempt in stamina.retry_context(
                            on=httpx.HTTPError,
                            attempts=3,
                            wait_initial=datetime.timedelta(seconds=10),
                            wait_max=datetime.timedelta(seconds=10),
                    ):
                        with attempt:
                            try:
                                response = await gundi_tools.send_observations_to_gundi(
                                    observations=transformed_data,
                                    integration_id=integration.id
                                )
                            except httpx.HTTPError as e:
                                msg = f'Sensors API returned error for integration_id: {str(integration.id)}. Exception: {e}'
                                logger.exception(
                                    msg,
                                    extra={
                                        'needs_attention': True,
                                        'integration_id': str(integration.id),
                                        'action_id': "pull_observations"
                                    }
                                )
                                response_per_device.append({"device": device, "response": [msg]})
                            else:
                                total_observations += len(transformed_data)
                                # Update state
                                state = {
                                    "latest_device_timestamp": transformed_data[-1].get("recorded_at")
                                }
                                await state_manager.set_state(
                                    str(integration.id),
                                    "pull_observations",
                                    state,
                                    device,
                                )
                                response_per_device.append({"device": device, "response": response})
                else:
                    response_per_device.append({"device": device, "response": []})
        else:
            msg = f'No observations extracted for integration_id: {str(integration.id)}.'
            logger.warning(msg)
            result["message"] = msg
    except httpx.HTTPError as e:
        message = f"pull_observations action returned error."
        logger.exception(message, extra={
            "integration_id": str(integration.id),
            "attention_needed": True
        })
        raise e
    else:
        result["observations_extracted"] = total_observations
        result["details"] = response_per_device
        return result
</file>

<file path="gundi-integration-tahmo-weather/app/routers/actions.py">
import logging
from typing import List
import app.settings
from fastapi import APIRouter, BackgroundTasks
from app.actions import get_actions
from app.services.action_runner import execute_action
from app.api_schemas import ActionRequest

logger = logging.getLogger(__name__)

router = APIRouter()


@router.get(
    "/",
    summary="Execute an action with given settings",
    response_model=List[str]
)
async def list_actions():
    return get_actions()

@router.post(
    "/execute",
    summary="Execute an action with given settings",
)
async def execute(
    request: ActionRequest,
    background_tasks: BackgroundTasks
):
    if request.run_in_background:
        background_tasks.add_task(
            execute_action,
            integration_id=request.integration_id,
            action_id=request.action_id
        )
        return {"message": "Action execution started in background"}
    else:
        return await execute_action(
            integration_id=request.integration_id,
            action_id=request.action_id
        )
</file>

<file path="gundi-integration-tahmo-weather/app/services/tests/test_action_runner.py">
import pytest
from fastapi.testclient import TestClient
from app.main import app


api_client = TestClient(app)


@pytest.mark.asyncio
async def test_execute_action_from_pubsub(
        mocker, mock_gundi_client_v2, mock_publish_event, mock_action_handlers,
        event_v2_cloud_event_headers, event_v2_cloud_event_payload
):
    mocker.patch("app.services.action_runner.action_handlers", mock_action_handlers)
    mocker.patch("app.services.action_runner._portal", mock_gundi_client_v2)
    mocker.patch("app.services.activity_logger.publish_event", mock_publish_event)

    response = api_client.post(
        "/",
        headers=event_v2_cloud_event_headers,
        json=event_v2_cloud_event_payload,
    )

    assert response.status_code == 200
    assert mock_gundi_client_v2.get_integration_details.called
    assert mock_action_handlers["pull_observations"].called


@pytest.mark.asyncio
async def test_execute_action_from_api(
        mocker, mock_gundi_client_v2, integration_v2,
        mock_publish_event, mock_action_handlers,
):
    mocker.patch("app.services.action_runner.action_handlers", mock_action_handlers)
    mocker.patch("app.services.action_runner._portal", mock_gundi_client_v2)
    mocker.patch("app.services.activity_logger.publish_event", mock_publish_event)

    response = api_client.post(
        "/v1/actions/execute/",
        json={
            "integration_id": str(integration_v2.id),
            "action_id": "pull_observations"
        }
    )

    assert response.status_code == 200
    assert mock_gundi_client_v2.get_integration_details.called
    assert mock_action_handlers["pull_observations"].called
</file>

<file path="gundi-integration-tahmo-weather/app/services/tests/test_activity_logger.py">
import pytest
from unittest.mock import ANY
from gundi_core.events import (
    LogLevel,
    IntegrationActionStarted,
    IntegrationActionComplete,
    IntegrationActionFailed,
    IntegrationActionCustomLog
)
from app import settings
from app.services.activity_logger import publish_event, activity_logger, log_activity


@pytest.mark.parametrize(
    "system_event",
    ["action_started_event", "action_complete_event", "action_failed_event", "custom_activity_log_event"],
    indirect=["system_event"])
@pytest.mark.asyncio
async def test_publish_event(
        mocker, mock_pubsub_client, integration_event_pubsub_message, gcp_pubsub_publish_response,
        system_event
):
    mocker.patch("app.services.activity_logger.pubsub", mock_pubsub_client)

    response = await publish_event(
        event=system_event,
        topic_name=settings.INTEGRATION_EVENTS_TOPIC
    )

    assert response == gcp_pubsub_publish_response
    assert mock_pubsub_client.PublisherClient.called
    assert mock_pubsub_client.PubsubMessage.called
    assert mock_pubsub_client.PublisherClient.called
    assert mock_pubsub_client.PublisherClient.return_value.publish.called
    mock_pubsub_client.PublisherClient.return_value.publish.assert_any_call(
        f"projects/{settings.GCP_PROJECT_ID}/topics/{settings.INTEGRATION_EVENTS_TOPIC}",
        [integration_event_pubsub_message],
    )


@pytest.mark.asyncio
async def test_activity_logger_decorator(
        mocker, mock_publish_event, integration_v2, pull_observations_config
):

    mocker.patch("app.services.activity_logger.publish_event", mock_publish_event)

    @activity_logger()
    async def action_pull_observations(integration, action_config):
        return {"observations_extracted": 10}

    await action_pull_observations(
        integration=integration_v2,
        action_config=pull_observations_config
    )

    # Two events expected: One on start and one on completion
    assert mock_publish_event.call_count == 2
    assert isinstance(mock_publish_event.call_args_list[0].kwargs.get("event"), IntegrationActionStarted)
    assert isinstance(mock_publish_event.call_args_list[1].kwargs.get("event"), IntegrationActionComplete)


@pytest.mark.asyncio
async def test_activity_logger_decorator_with_arguments(
        mocker, mock_publish_event, integration_v2, pull_observations_config
):

    mocker.patch("app.services.activity_logger.publish_event", mock_publish_event)

    @activity_logger(on_start=False, on_completion=True, on_error=False)
    async def action_pull_observations(integration, action_config):
        return {"observations_extracted": 10}

    await action_pull_observations(
        integration=integration_v2,
        action_config=pull_observations_config
    )

    # Only one event expected, on completion
    assert mock_publish_event.call_count == 1
    assert isinstance(mock_publish_event.call_args_list[0].kwargs.get("event"), IntegrationActionComplete)


@pytest.mark.asyncio
async def test_activity_logger_decorator_on_error(
        mocker, mock_publish_event, integration_v2, pull_observations_config
):

    mocker.patch("app.services.activity_logger.publish_event", mock_publish_event)

    @activity_logger()
    async def action_pull_observations(integration, action_config):
        raise Exception("Something went wrong")

    with pytest.raises(Exception):
        await action_pull_observations(
            integration=integration_v2,
            action_config=pull_observations_config
        )

    # Two events expected: One on start and one on error
    assert mock_publish_event.call_count == 2
    assert isinstance(mock_publish_event.call_args_list[0].kwargs.get("event"), IntegrationActionStarted)
    assert isinstance(mock_publish_event.call_args_list[1].kwargs.get("event"), IntegrationActionFailed)


@pytest.mark.asyncio
async def test_log_activity_with_debug_level(mocker, integration_v2, pull_observations_config, mock_publish_event):
    mocker.patch("app.services.activity_logger.publish_event", mock_publish_event)
    await log_activity(
        integration_id=integration_v2.id,
        action_id="pull_observations",
        level=LogLevel.DEBUG,
        title="Extracted 10 observations from 2 devices",
        data={"devices": ["deviceid1", "deviceid2"]},
        config_data=pull_observations_config.data
    )
    assert mock_publish_event.call_count == 1
    assert isinstance(mock_publish_event.call_args_list[0].kwargs.get("event"), IntegrationActionCustomLog)


@pytest.mark.asyncio
async def test_log_activity_with_info_level(mocker, integration_v2, mock_publish_event, pull_observations_config):
    mocker.patch("app.services.activity_logger.publish_event", mock_publish_event)
    await log_activity(
        integration_id=integration_v2.id,
        action_id="pull_observations",
        level=LogLevel.INFO,
        title="Extracting observations with filter..",
        data={"start_date": "2024-01-01", "end_date": "2024-01-31"},
        config_data=pull_observations_config.data
    )
    assert mock_publish_event.call_count == 1
    assert isinstance(mock_publish_event.call_args_list[0].kwargs.get("event"), IntegrationActionCustomLog)


@pytest.mark.asyncio
async def test_log_activity_with_warning_level(mocker, integration_v2, mock_publish_event, pull_observations_config):
    mocker.patch("app.services.activity_logger.publish_event", mock_publish_event)
    await log_activity(
        integration_id=integration_v2.id,
        action_id="pull_observations",
        level=LogLevel.WARNING,
        title="Skipping end_date because it's greater than today. Please review your configuration.",
        config_data=pull_observations_config.data
    )
    assert mock_publish_event.call_count == 1
    assert isinstance(mock_publish_event.call_args_list[0].kwargs.get("event"), IntegrationActionCustomLog)


@pytest.mark.asyncio
async def test_log_activity_with_error_level(mocker, integration_v2, mock_publish_event, pull_observations_config):
    mocker.patch("app.services.activity_logger.publish_event", mock_publish_event)
    await log_activity(
        integration_id=integration_v2.id,
        action_id="pull_observations",
        level=LogLevel.ERROR,
        title="Error getting data from System X",
        data={"error": "Connection error with host 'systemx.com'"},
        config_data=pull_observations_config.data
    )
    assert mock_publish_event.call_count == 1
    assert isinstance(mock_publish_event.call_args_list[0].kwargs.get("event"), IntegrationActionCustomLog)
</file>

<file path="gundi-integration-tahmo-weather/app/services/tests/test_gundi_api.py">
import pytest
from app.services.gundi import send_events_to_gundi, send_observations_to_gundi


@pytest.mark.asyncio
async def test_send_events_to_gundi(
        mocker, mock_gundi_client_v2_class, mock_gundi_sensors_client_class,
        mock_get_gundi_api_key, integration_v2
):
    mocker.patch("app.services.gundi.GundiClient", mock_gundi_client_v2_class)
    mocker.patch("app.services.gundi.GundiDataSenderClient", mock_gundi_sensors_client_class)
    mocker.patch("app.services.gundi._get_gundi_api_key", mock_get_gundi_api_key)
    events = [
        {
            "title": "Animal Sighting",
            "event_type": "wildlife_sighting_rep",
            "recorded_at": "2024-01-08 21:51:10-03:00",
            "location": {
                "lat": -51.688645,
                "lon": -72.704421
            },
            "event_details": {
                "site_name": "MM Spot",
                "species": "lion"
            }
        },
        {
            "title": "Animal Sighting",
            "event_type": "wildlife_sighting_rep",
            "recorded_at": "2024-01-08 21:51:10-03:00",
            "location": {
                "lat": -51.688645,
                "lon": -72.704421
            },
            "event_details": {
                "site_name": "MM Spot",
                "species": "lion"
            }
        }
    ]
    response = await send_events_to_gundi(
        events=events,
        integration_id=integration_v2.id
    )

    # Data is sent to gundi using the REST API for now
    assert len(response) == 2
    assert mock_gundi_sensors_client_class.called
    mock_gundi_sensors_client_class.return_value.post_events.assert_called_once_with(data=events)


@pytest.mark.asyncio
async def test_send_observations_to_gundi(
        mocker, mock_gundi_client_v2_class, mock_gundi_sensors_client_class,
        mock_get_gundi_api_key, integration_v2
):
    mocker.patch("app.services.gundi.GundiClient", mock_gundi_client_v2_class)
    mocker.patch("app.services.gundi.GundiDataSenderClient", mock_gundi_sensors_client_class)
    mocker.patch("app.services.gundi._get_gundi_api_key", mock_get_gundi_api_key)
    observations = [
        {
            "source": "device-xy123",
            "type": "tracking-device",
            "subject_type": "puma",
            "recorded_at": "2024-01-24 09:03:00-0300",
            "location": {
                "lat": -51.748,
                "lon": -72.720
            },
            "additional": {
                "speed_kmph": 5
            }
        },
        {
            "source": "test-device-mariano",
            "type": "tracking-device",
            "subject_type": "puma",
            "recorded_at": "2024-01-24 09:05:00-0300",
            "location": {
                "lat": -51.755,
                "lon": -72.755
            },
            "additional": {
                "speed_kmph": 5
            }
        }
    ]
    response = await send_observations_to_gundi(
        observations=observations,
        integration_id=integration_v2.id
    )

    # Data is sent to gundi using the REST API for now
    assert len(response) == 2
    assert mock_gundi_sensors_client_class.called
    mock_gundi_sensors_client_class.return_value.post_observations.assert_called_once_with(data=observations)
</file>

<file path="gundi-integration-tahmo-weather/app/services/tests/test_state_manager.py">
import datetime
import json

import pytest
from app.services.state import IntegrationStateManager


@pytest.mark.asyncio
async def test_set_integration_state(mocker, mock_redis, integration_v2):
    mocker.patch("app.services.state.redis", mock_redis)
    state_manager = IntegrationStateManager()
    execution_timestamp = datetime.datetime.now(tz=datetime.timezone.utc).isoformat()
    integration_id = str(integration_v2.id)
    state = {"last_execution": execution_timestamp}

    await state_manager.set_state(
        integration_id=integration_id,
        action_id="pull_observations",
        # No source set
        state=state
    )

    mock_redis.Redis.return_value.set.assert_called_once_with(
        f"integration_state.{integration_id}.pull_observations.no-source",
        '{"last_execution": "' + execution_timestamp + '"}'
    )


@pytest.mark.asyncio
async def test_get_integration_state(mocker, mock_redis, integration_v2, mock_integration_state):
    mocker.patch("app.services.state.redis", mock_redis)
    state_manager = IntegrationStateManager()
    integration_id = str(integration_v2.id)

    state = await state_manager.get_state(
        integration_id=integration_id,
        action_id="pull_observations",
        # No source set
    )

    assert state == mock_integration_state
    mock_redis.Redis.return_value.get.assert_called_once_with(
        f"integration_state.{integration_id}.pull_observations.no-source"
    )


@pytest.mark.asyncio
async def test_delete_integration_state(mocker, mock_redis, integration_v2):
    mocker.patch("app.services.state.redis", mock_redis)
    state_manager = IntegrationStateManager()

    execution_timestamp = datetime.datetime.now(tz=datetime.timezone.utc).isoformat()
    integration_id = str(integration_v2.id)

    # set state
    state = {"last_execution": execution_timestamp}

    await state_manager.set_state(
        integration_id=integration_id,
        action_id="pull_observations",
        # No source set
        state=state
    )

    mock_redis.Redis.return_value.set.assert_called_once_with(
        f"integration_state.{integration_id}.pull_observations.no-source",
        '{"last_execution": "' + execution_timestamp + '"}'
    )

    # then delete the state

    await state_manager.delete_state(
        integration_id=integration_id,
        action_id="pull_observations",
        # No source set
    )

    mock_redis.Redis.return_value.delete.assert_called_once_with(
        f"integration_state.{integration_id}.pull_observations.no-source"
    )


@pytest.mark.asyncio
async def test_set_source_state(mocker, mock_redis, integration_v2, mock_integration_state):
    mocker.patch("app.services.state.redis", mock_redis)
    state_manager = IntegrationStateManager()
    integration_id = str(integration_v2.id)
    source_id = "device-123"

    await state_manager.set_state(
        integration_id=integration_id,
        action_id="pull_observations",
        source_id=source_id,
        state=mock_integration_state
    )

    mock_redis.Redis.return_value.set.assert_called_once_with(
        f"integration_state.{integration_id}.pull_observations.{source_id}",
        json.dumps(mock_integration_state, default=str)
    )


@pytest.mark.asyncio
async def test_get_state_source_state(mocker, mock_redis, integration_v2, mock_integration_state):
    mocker.patch("app.services.state.redis", mock_redis)
    state_manager = IntegrationStateManager()
    integration_id = str(integration_v2.id)
    source_id = "device-123"

    state = await state_manager.get_state(
        integration_id=integration_id,
        action_id="pull_observations",
        source_id=source_id
    )

    assert state == mock_integration_state
    mock_redis.Redis.return_value.get.assert_called_once_with(
        f"integration_state.{integration_id}.pull_observations.{source_id}"
    )


@pytest.mark.asyncio
async def test_delete_state_source_state(mocker, mock_redis, integration_v2, mock_integration_state):
    mocker.patch("app.services.state.redis", mock_redis)
    state_manager = IntegrationStateManager()
    integration_id = str(integration_v2.id)
    source_id = "device-123"

    # set state
    await state_manager.set_state(
        integration_id=integration_id,
        action_id="pull_observations",
        source_id=source_id,
        state=mock_integration_state
    )

    mock_redis.Redis.return_value.set.assert_called_once_with(
        f"integration_state.{integration_id}.pull_observations.{source_id}",
        json.dumps(mock_integration_state, default=str)
    )

    # delete state

    await state_manager.delete_state(
        integration_id=integration_id,
        action_id="pull_observations",
        source_id=source_id
    )

    mock_redis.Redis.return_value.delete.assert_called_once_with(
        f"integration_state.{integration_id}.pull_observations.{source_id}"
    )
</file>

<file path="gundi-integration-tahmo-weather/app/services/action_runner.py">
import datetime
import logging
import httpx
import pydantic
import stamina
from gundi_client_v2 import GundiClient
from app.actions import action_handlers
from app import settings
from fastapi import status
from fastapi.encoders import jsonable_encoder
from fastapi.responses import JSONResponse
from gundi_core.events import IntegrationActionFailed, ActionExecutionFailed
from .utils import find_config_for_action
from .activity_logger import publish_event


_portal = GundiClient()
logger = logging.getLogger(__name__)


async def execute_action(integration_id: str, action_id: str, config_overrides: dict = None):
    """
    Interface for executing actions.
    :param integration_id: The UUID of the integration
    :param action_id: "test_auth", "pull_observations", "pull_events"
    :param config_overrides: Optional dictionary with configuration overrides
    :return: action result if any, or raise an exception
    """
    logger.info(f"Executing action '{action_id}' for integration '{integration_id}'...")
    try:  # Get the integration config from the portal
        async for attempt in stamina.retry_context(on=httpx.HTTPError, wait_initial=1.0, wait_jitter=5.0, wait_max=32.0):
            with attempt:
                # ToDo: Store configs and update it on changes (event-driven architecture)
                integration = await _portal.get_integration_details(integration_id=integration_id)
    except Exception as e:
        message = f"Error retrieving configuration for integration '{integration_id}': {e}"
        logger.exception(message)
        await publish_event(
            event=IntegrationActionFailed(
                payload=ActionExecutionFailed(
                    integration_id=integration_id,
                    action_id=action_id,
                    error=message
                )
            ),
            topic_name=settings.INTEGRATION_EVENTS_TOPIC,
        )
        return JSONResponse(
            status_code=e.response.status_code if hasattr(e, "response") else status.HTTP_500_INTERNAL_SERVER_ERROR,
            content=jsonable_encoder({"detail": message}),
        )

    # Look for the configuration of the action being executed
    action_config = find_config_for_action(
        configurations=integration.configurations,
        action_id=action_id
    )
    if not action_config and not config_overrides:
        message = f"Configuration for action '{action_id}' for integration {str(integration.id)} " \
                  f"is missing. Please fix the integration setup in the portal or provide a config in the request."
        logger.error(message)
        await publish_event(
            event=IntegrationActionFailed(
                payload=ActionExecutionFailed(
                    integration_id=integration_id,
                    action_id=action_id,
                    error=f"Configuration missing for action '{action_id}'",
                    config_data={"configurations": [i.dict() for i in integration.configurations]},
                )
            ),
            topic_name=settings.INTEGRATION_EVENTS_TOPIC,
        )
        return JSONResponse(
            status_code=status.HTTP_404_NOT_FOUND,
            content=jsonable_encoder({"detail": message}),
        )
    try:  # Execute the action
        handler, config_model = action_handlers[action_id]
        config_data = action_config.data if action_config else {}
        if config_overrides:
            config_data.update(config_overrides)
        parsed_config = config_model.parse_obj(config_data)
        result = await handler(integration=integration, action_config=parsed_config)
    except pydantic.ValidationError as e:
        message = f"Invalid configuration for action '{action_id}' and integration '{integration_id}': {e.errors()}"
        logger.error(message)
        await publish_event(
            event=IntegrationActionFailed(
                payload=ActionExecutionFailed(
                    integration_id=integration_id,
                    action_id=action_id,
                    config_data=config_data,
                    error=message
                )
            ),
            topic_name=settings.INTEGRATION_EVENTS_TOPIC,
        )
        return JSONResponse(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            content=jsonable_encoder({"detail": message}),
        )
    except KeyError as e:
        message = f"Action '{action_id}' is not supported for this integration"
        logger.exception(message)
        await publish_event(
            event=IntegrationActionFailed(
                payload=ActionExecutionFailed(
                    integration_id=integration_id,
                    action_id=action_id,
                    config_data=action_config,
                    error=message
                )
            ),
            topic_name=settings.INTEGRATION_EVENTS_TOPIC,
        )
        return JSONResponse(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            content=jsonable_encoder({"detail": message}),
        )
    except Exception as e:
        message = f"Internal error executing action '{action_id}': {e}"
        logger.exception(message)
        await publish_event(
            event=IntegrationActionFailed(
                payload=ActionExecutionFailed(
                    integration_id=integration_id,
                    action_id=action_id,
                    config_data={"configurations": [c.dict() for c in integration.configurations]},
                    error=message
                )
            ),
            topic_name=settings.INTEGRATION_EVENTS_TOPIC,
        )
        return JSONResponse(
            status_code=e.response.status_code if hasattr(e, "response") else status.HTTP_500_INTERNAL_SERVER_ERROR,
            content=jsonable_encoder({"detail": message}),
        )
    else:
        return result
</file>

<file path="gundi-integration-tahmo-weather/app/services/activity_logger.py">
import asyncio
import json
import logging

import aiohttp
import stamina
from functools import wraps
from gcloud.aio import pubsub
from gundi_core.events import (
    SystemEventBaseModel,
    IntegrationActionCustomLog,
    CustomActivityLog,
    IntegrationActionStarted,
    ActionExecutionStarted,
    IntegrationActionFailed,
    ActionExecutionFailed,
    IntegrationActionComplete,
    ActionExecutionComplete,
    IntegrationWebhookCustomLog,
    IntegrationWebhookStarted,
    WebhookExecutionStarted,
    IntegrationWebhookComplete,
    WebhookExecutionComplete,
    IntegrationWebhookFailed,
    WebhookExecutionFailed,
    CustomWebhookLog,
)
from app import settings


logger = logging.getLogger(__name__)


# Publish events for other services or system components
@stamina.retry(
    on=(aiohttp.ClientError, asyncio.TimeoutError),
    attempts=5,
    wait_initial=1.0,
    wait_max=30,
    wait_jitter=3.0
)
async def publish_event(event: SystemEventBaseModel, topic_name: str):
    timeout_settings = aiohttp.ClientTimeout(total=10.0)
    async with aiohttp.ClientSession(
        raise_for_status=True, timeout=timeout_settings
    ) as session:
        client = pubsub.PublisherClient(session=session)
        # Get the topic
        topic = client.topic_path(settings.GCP_PROJECT_ID, topic_name)
        # Prepare the payload
        binary_payload = json.dumps(event.dict(), default=str).encode("utf-8")
        messages = [pubsub.PubsubMessage(binary_payload)]
        logger.debug(f"Sending event {event} to PubSub topic {topic_name}..")
        try:  # Send to pubsub
            response = await client.publish(topic, messages)
        except Exception as e:
            logger.exception(
                f"Error publishing system event to topic {topic_name}: {e}. This will be retried."
            )
            raise e
        else:
            logger.debug(f"System event {event} published successfully.")
            logger.debug(f"GCP PubSub response: {response}")
            return response


async def log_activity(integration_id: str, action_id: str, title: str, level="INFO", config_data: dict = None, data: dict = None):
    # Show a deprecation warning in favor of using either log_action_activity or log_webhook_activity
    logger.warning("log_activity is deprecated. Please use log_action_activity or log_webhook_activity instead.")
    return await log_action_activity(integration_id, action_id, title, level, config_data, data)


async def log_action_activity(integration_id: str, action_id: str, title: str, level="INFO", config_data: dict = None, data: dict = None):
    """
        This is a helper method to send custom activity logs to the portal.
        :param integration_id: UUID of the integration
        :param action_id: str id of the action being executed
        :param title: A human-readable string that will appear in the activity log
        :param level: The level of the log, e.g. DEBUG, INFO, WARNING, ERROR
        :param data: Any extra data to be logged as a dict
        :return: None
        """
    logger.debug(f"Logging custom activity: {title}. Integration: {integration_id}. Action: {action_id}.")
    await publish_event(
        event=IntegrationActionCustomLog(
            payload=CustomActivityLog(
                integration_id=integration_id,
                action_id=action_id,
                config_data=config_data or {},
                title=title,
                level=level,
                data=data
            )
        ),
        topic_name=settings.INTEGRATION_EVENTS_TOPIC,
    )


async def log_webhook_activity(
        integration_id: str, title: str, webhook_id: str="webhook", level="INFO", config_data: dict = None, data: dict = None
):
    """
        This is a helper method to send custom activity logs to the portal.
        :param integration_id: UUID of the integration
        :param title: A human-readable string that will appear in the activity log
        :param webhook_id: str id of the webhook being executed
        :param level: The level of the log, e.g. DEBUG, INFO, WARNING, ERROR
        :param data: Any extra data to be logged as a dict
        :return: None
        """
    logger.debug(f"Logging custom activity: {title}. Integration: {integration_id}. Webhook: {webhook_id}.")
    await publish_event(
        event=IntegrationWebhookCustomLog(
            payload=CustomWebhookLog(
                integration_id=integration_id,
                webhook_id=webhook_id,
                config_data=config_data or {},
                title=title,
                level=level,
                data=data
            )
        ),
        topic_name=settings.INTEGRATION_EVENTS_TOPIC,
    )


def activity_logger(on_start=True, on_completion=True, on_error=True):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            integration = kwargs.get("integration")
            integration_id = str(integration.id) if integration else None
            action_id = func.__name__.replace("action_", "")
            action_config = kwargs.get("action_config")
            config_data = action_config.dict() if action_config else {} or {}
            if on_start:
                await publish_event(
                    event=IntegrationActionStarted(
                        payload=ActionExecutionStarted(
                            integration_id=integration_id,
                            action_id=action_id,
                            config_data=config_data,
                        )
                    ),
                    topic_name=settings.INTEGRATION_EVENTS_TOPIC,
                )
            try:
                result = await func(*args, **kwargs)
            except Exception as e:
                if on_error:
                    await publish_event(
                        event=IntegrationActionFailed(
                            payload=ActionExecutionFailed(
                                integration_id=integration_id,
                                action_id=action_id,
                                config_data=config_data,
                                error=str(e)
                            )
                        ),
                        topic_name=settings.INTEGRATION_EVENTS_TOPIC,
                    )
                raise e
            else:
                if on_completion:
                    await publish_event(
                        event=IntegrationActionComplete(
                            payload=ActionExecutionComplete(
                                integration_id=integration_id,
                                action_id=action_id,
                                config_data=config_data,
                                result=result
                            )
                        ),
                        topic_name=settings.INTEGRATION_EVENTS_TOPIC,
                    )
                return result
        return wrapper
    return decorator


def webhook_activity_logger(on_start=True, on_completion=True, on_error=True):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            integration = kwargs.get("integration")
            integration_id = str(integration.id) if integration else None
            webhook_config = kwargs.get("webhook_config")
            config_data = webhook_config.dict() if webhook_config else {} or {}
            webhook_id = str(integration.webhook_configuration.webhook.value) if integration and integration.webhook_configuration else "webhook"
            if on_start:
                await publish_event(
                    event=IntegrationWebhookStarted(
                        payload=WebhookExecutionStarted(
                            integration_id=integration_id,
                            webhook_id=webhook_id,
                            config_data=config_data,
                        )
                    ),
                    topic_name=settings.INTEGRATION_EVENTS_TOPIC,
                )
            try:
                result = await func(*args, **kwargs)
            except Exception as e:
                if on_error:
                    await publish_event(
                        event=IntegrationWebhookFailed(
                            payload=WebhookExecutionFailed(
                                integration_id=integration_id,
                                webhook_id=webhook_id,
                                config_data=config_data,
                                error=str(e)
                            )
                        ),
                        topic_name=settings.INTEGRATION_EVENTS_TOPIC,
                    )
                raise e
            else:
                if on_completion:
                    await publish_event(
                        event=IntegrationWebhookComplete(
                            payload=WebhookExecutionComplete(
                                integration_id=integration_id,
                                webhook_id=webhook_id,
                                config_data=config_data,
                                result=result
                            )
                        ),
                        topic_name=settings.INTEGRATION_EVENTS_TOPIC,
                    )
                return result
        return wrapper
    return decorator
</file>

<file path="gundi-integration-tahmo-weather/app/services/errors.py">
class ActionNotFound(Exception):
    pass


class ConfigurationNotFound(Exception):
    pass


class ConfigurationValidationError(Exception):
    pass


class ActionExecutionError(Exception):
    pass
</file>

<file path="gundi-integration-tahmo-weather/app/services/gundi.py">
import datetime
from typing import List
import httpx
import stamina
from gundi_client_v2.client import GundiClient, GundiDataSenderClient


@stamina.retry(on=httpx.HTTPError, attempts=3, wait_initial=datetime.timedelta(seconds=1), wait_max=datetime.timedelta(seconds=10))
async def _get_gundi_api_key(integration_id):
    async with GundiClient() as gundi_client:
        return await gundi_client.get_integration_api_key(
            integration_id=integration_id
        )


async def _get_sensors_api_client(integration_id):
    gundi_api_key = await _get_gundi_api_key(integration_id=integration_id)
    assert gundi_api_key, f"Cannot get a valid API Key for integration {integration_id}"
    sensors_api_client = GundiDataSenderClient(
        integration_api_key=gundi_api_key
    )
    return sensors_api_client


@stamina.retry(on=httpx.HTTPError, attempts=3, wait_initial=datetime.timedelta(seconds=1), wait_max=datetime.timedelta(seconds=10))
async def send_events_to_gundi(events: List[dict], **kwargs) -> dict:
    """
    Send Events to Gundi using the REST API v2
    :param events: A list of events in the following format:
    [
        {
        "title": "Animal Sighting",
        "event_type": "wildlife_sighting_rep",
        "recorded_at":"2024-01-08 21:51:10-03:00",
        "location":{
            "lat":-51.688645,
            "lon":-72.704421
        },
        "event_details":{
            "site_name":"MM Spot",
            "species":"lion"
        },
        ...
    ]
    :param kwargs: integration_id: The UUID of the related integration
    :return: A dict with the response from the API
    """
    integration_id = kwargs.get("integration_id")
    assert integration_id, "integration_id is required"
    sensors_api_client = await _get_sensors_api_client(integration_id=str(integration_id))
    return await sensors_api_client.post_events(data=events)


@stamina.retry(on=httpx.HTTPError, attempts=3, wait_initial=datetime.timedelta(seconds=1), wait_max=datetime.timedelta(seconds=10))
async def send_observations_to_gundi(observations: List[dict], **kwargs) -> dict:
    """
    Send Observations to Gundi using the REST API v2
    :param observations: A list of observations in the following format:
    [
        {
            "source": "collar-xy123",
            "type": "tracking-device",
            "subject_type": "puma",
            "recorded_at": "2024-01-24 09:03:00-0300",
            "location": {
                "lat": -51.748,
                "lon": -72.720
            },
            "additional": {
                "speed_kmph": 10
            }
        },
        ...
    ]
    :param kwargs: integration_id: The UUID of the related integration
    :return: A dict with the response from the API
    """
    integration_id = kwargs.get("integration_id")
    assert integration_id, "integration_id is required"
    sensors_api_client = await _get_sensors_api_client(integration_id=str(integration_id))
    return await sensors_api_client.post_observations(data=observations)
</file>

<file path="gundi-integration-tahmo-weather/app/services/state.py">
import json
import redis.asyncio as redis
from app import settings


class IntegrationStateManager:

    def __init__(self, **kwargs):
        host = kwargs.get("host", settings.REDIS_HOST)
        port = kwargs.get("port", settings.REDIS_PORT)
        db = kwargs.get("db", settings.REDIS_STATE_DB)
        self.db_client = redis.Redis(host=host, port=port, db=db)

    async def get_state(self, integration_id: str, action_id: str, source_id: str = "no-source") -> dict:
        json_value = await self.db_client.get(f"integration_state.{integration_id}.{action_id}.{source_id}")
        value = json.loads(json_value) if json_value else {}
        return value

    async def set_state(self, integration_id: str, action_id: str, state: dict, source_id: str = "no-source"):
        await self.db_client.set(
            f"integration_state.{integration_id}.{action_id}.{source_id}",
            json.dumps(state, default=str)
        )

    async def delete_state(self, integration_id: str, action_id: str, source_id: str = "no-source"):
        await self.db_client.delete(
            f"integration_state.{integration_id}.{action_id}.{source_id}"
        )

    def __str__(self):
        return f"IntegrationStateManager(host={self.db_client.host}, port={self.db_client.port}, db={self.db_client.db})"

    def __repr__(self):
        return self.__str__()
</file>

<file path="gundi-integration-tahmo-weather/app/services/utils.py">
import struct
from typing import Annotated
import typing
from pydantic import create_model, BaseModel
from pydantic.fields import Field, FieldInfo, Undefined, NoArgAnyCallable
from typing import Any, Dict, Optional, Union, List


def find_config_for_action(configurations, action_id):
    return next(
        (
            config for config in configurations
            if config.action.value == action_id
        ),
        None
    )


class StructHexString:
    def __init__(self, value: str, hex_format):
        self.value = value
        self.hex_format = hex_format
        self.format_spec = hex_format.get("byte_order", "<") + ''.join(f["format"] for f in hex_format["fields"])
        self.unpacked_data = self._unpack_data()

    @classmethod
    def __get_validators__(cls):
        yield cls.validate

    @classmethod
    def validate(cls, v: str, values, field):
        hex_format = values['hex_format']  # Assumes format is already set in the parent model
        format_spec = hex_format.get("byte_order", "<") + ''.join(d["format"] for d in hex_format["fields"])
        try:
            bytes_data = bytes.fromhex(v)
            if len(bytes_data) != struct.calcsize(format_spec):
                raise ValueError("Hex string does not match the expected length for format")
        except (ValueError, struct.error) as e:
            raise ValueError(f"Invalid hex string for format '{format_spec}': {str(e)}")

        return cls(v, hex_format)

    @classmethod
    def __modify_schema__(cls, field_schema):
        field_schema.update(type="hex_string", example="123456789ABCDEF", description="Hex string data")

    def _unpack_data(self):
        field_values = []
        unpacked_fields = struct.unpack(self.format_spec, bytes.fromhex(self.value))
        for s, v in zip(self.hex_format["fields"], unpacked_fields):
            field_values.append(self._cast_output(value=v, output_type=s.get("output_type", "int")))
        field_names = [f["name"] for f in self.hex_format["fields"]]
        fields_with_bitfields = [f for f in self.hex_format["fields"] if "bit_fields" in f]
        for field in fields_with_bitfields:
            bit_fields = field["bit_fields"]
            for bit_field in bit_fields:
                start_bit = bit_field["start_bit"]
                end_bit = bit_field["end_bit"]
                field_value = unpacked_fields[field_names.index(field["name"])]
                bits_value = (field_value >> start_bit) & (2 ** (end_bit - start_bit + 1) - 1)
                field_values.append(self._cast_output(value=bits_value, output_type=bit_field.get("output_type", "bool")))
                field_names.append(bit_field["name"])
        return dict(zip(field_names, field_values))

    def _cast_output(self, value, output_type="hex"):
        if output_type == "bool":
            return bool(value)
        elif output_type == "int":
            return int(value)
        else:  # hex string by default
            return hex(value)

    def __repr__(self) -> str:
        return f"StructHexString(value={self.value}, hex_format={self.hex_format})"

    def to_dict(self) -> typing.Dict[str, typing.Any]:
        return {
            "value": self.value,
            "hex_format": self.hex_format,
            "unpacked_data": self.unpacked_data
        }


Model = typing.TypeVar('Model', bound='BaseModel')


class DyntamicFactory:
    """
    Modified version of the DyntamicFactory class from:
    https://github.com/c32168/dyntamic
    """

    TYPES = {
        'string': str,
        'array': list,
        'boolean': bool,
        'integer': int,
        'float': float,
        'number': float,
        'object': dict,
        'hex_string': StructHexString
        # ToDo: test with custom types such as StructHexString
    }

    def __init__(self,
                 json_schema: dict,
                 base_model: type[Model] | tuple[type[Model], ...] | None = None,
                 ref_template: str = "#/$defs/"
                 ) -> None:
        """
        Creates a dynamic pydantic model from a JSONSchema, dumped from and existing Pydantic model elsewhere.
            JSONSchema dump must be called with ref_template='{model}' like:

            SomeSampleModel.model_json_schema(ref_template='{model}')
            Use:
            >> _factory = DyntamicFactory(schema)
            >> _factory.make()
            >> _model = create_model(_factory.class_name, **_factory.model_fields)
            >> _instance = dynamic_model.model_validate(json_with_data)
            >> validated_data = model_instance.model_dump()
        """
        self.class_name = json_schema.get('title', "DataSchema")
        self.class_type = json_schema.get('type')
        self.required = json_schema.get('required', [])
        self.raw_fields = json_schema.get('properties', [])
        self.ref_template = ref_template
        self.definitions = json_schema.get(ref_template)
        self.fields = {}
        self.model_fields = {}
        self._base_model = base_model

    def make(self) -> Model:
        """Factory method, dynamically creates a pydantic model from JSON Schema"""
        for field in self.raw_fields:
            if '$ref' in self.raw_fields[field]:
                model_name = self.raw_fields[field].get('$ref')
                self._make_nested(model_name, field)
            else:
                factory = self.TYPES.get(self.raw_fields[field].get('type'))
                if factory == list:
                    items = self.raw_fields[field].get('items')
                    if self.ref_template in items:
                        self._make_nested(items.get(self.ref_template), field)
                self._make_field(factory, field, self.raw_fields.get('title'))
        return create_model(self.class_name, __base__=self._base_model, **self.model_fields)

    def _make_nested(self, model_name: str, field) -> None:
        """Create a nested model"""
        clean_model_name = model_name.split("/")[-1].strip()
        level = DyntamicFactory({self.ref_template: self.definitions} | self.definitions.get(clean_model_name),
                                ref_template=self.ref_template)
        level.make()
        model = create_model(clean_model_name, **level.model_fields)
        self._make_field(model, field, field)

    def _make_field(self, factory, field, alias) -> None:
        """Create an annotated field"""
        if field not in self.required:
            factory_annotation = Annotated[Union[factory | None], factory]
            self.model_fields[field] = (
                Annotated[factory_annotation, Field(default_factory=factory, alias=alias)],
                ...
            )
        else:
            self.model_fields[field] = (
                Annotated[factory, Field(..., alias=alias)],
                ...
            )


class GlobalUISchemaOptions(BaseModel):
    order: Optional[List[str]]
    addable: Optional[bool]
    copyable: Optional[bool]
    orderable: Optional[bool]
    removable: Optional[bool]
    label: Optional[bool]
    duplicateKeySuffixSeparator: Optional[str]


class UIOptions(GlobalUISchemaOptions):
    classNames: Optional[str]
    style: Optional[Dict[str, Any]]  # Assuming style is a dictionary of CSS properties
    title: Optional[str]
    description: Optional[str]
    placeholder: Optional[str]
    help: Optional[str]
    autofocus: Optional[bool]
    autocomplete: Optional[str]  # Type of HTMLInputElement['autocomplete']
    disabled: Optional[bool]
    emptyValue: Optional[Any]
    enumDisabled: Optional[Union[List[Union[str, int, bool]], None]]  # List of disabled enum options
    hideError: Optional[bool]
    readonly: Optional[bool]
    filePreview: Optional[bool]
    inline: Optional[bool]
    inputType: Optional[str]
    rows: Optional[int]
    submitButtonOptions: Optional[Dict[str, Any]]  # Assuming UISchemaSubmitButtonOptions is a dict
    widget: Optional[Union[str, Any]]  # Either a widget implementation or its name
    enumNames: Optional[List[str]]  # List of labels for enum values


class FieldInfoWithUIOptions(FieldInfo):

    def __init__(self, *args, **kwargs):
        """
        Extends the Pydantic Field class to support ui:schema generation
        :param kwargs: ui_options: UIOptions
        """
        self.ui_options = kwargs.pop("ui_options", None)
        super().__init__(*args, **kwargs)

    def ui_schema(self, *args, **kwargs):
        """Generates a UI schema from model field ui_options"""
        if not self.ui_options:
            return {}
        ui_schema = {}
        ui_options = self.ui_options.__fields__
        for field_name, model_field in ui_options.items():
            if value := getattr(self.ui_options, field_name, model_field.default):
                ui_schema[f"ui:{field_name}"] = value
        return ui_schema


def FieldWithUIOptions(
    default: Any = Undefined,
    *,
    default_factory: Optional[NoArgAnyCallable] = None,
    alias: Optional[str] = None,
    title: Optional[str] = None,
    description: Optional[str] = None,
    exclude: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny', Any]] = None,
    include: Optional[Union['AbstractSetIntStr', 'MappingIntStrAny', Any]] = None,
    const: Optional[bool] = None,
    gt: Optional[float] = None,
    ge: Optional[float] = None,
    lt: Optional[float] = None,
    le: Optional[float] = None,
    multiple_of: Optional[float] = None,
    allow_inf_nan: Optional[bool] = None,
    max_digits: Optional[int] = None,
    decimal_places: Optional[int] = None,
    min_items: Optional[int] = None,
    max_items: Optional[int] = None,
    unique_items: Optional[bool] = None,
    min_length: Optional[int] = None,
    max_length: Optional[int] = None,
    allow_mutation: bool = True,
    regex: Optional[str] = None,
    discriminator: Optional[str] = None,
    repr: bool = True,
    ui_options: UIOptions = None,
    **extra: Any,
) -> FieldInfoWithUIOptions:
    """
    Used to provide extra information about a field, either for the model schema or complex validation. Some arguments
    apply only to number fields (``int``, ``float``, ``Decimal``) and some apply only to ``str``.

    :param default: since this is replacing the field’s default, its first argument is used
      to set the default, use ellipsis (``...``) to indicate the field is required
    :param default_factory: callable that will be called when a default value is needed for this field
      If both `default` and `default_factory` are set, an error is raised.
    :param alias: the public name of the field
    :param title: can be any string, used in the schema
    :param description: can be any string, used in the schema
    :param exclude: exclude this field while dumping.
      Takes same values as the ``include`` and ``exclude`` arguments on the ``.dict`` method.
    :param include: include this field while dumping.
      Takes same values as the ``include`` and ``exclude`` arguments on the ``.dict`` method.
    :param const: this field is required and *must* take it's default value
    :param gt: only applies to numbers, requires the field to be "greater than". The schema
      will have an ``exclusiveMinimum`` validation keyword
    :param ge: only applies to numbers, requires the field to be "greater than or equal to". The
      schema will have a ``minimum`` validation keyword
    :param lt: only applies to numbers, requires the field to be "less than". The schema
      will have an ``exclusiveMaximum`` validation keyword
    :param le: only applies to numbers, requires the field to be "less than or equal to". The
      schema will have a ``maximum`` validation keyword
    :param multiple_of: only applies to numbers, requires the field to be "a multiple of". The
      schema will have a ``multipleOf`` validation keyword
    :param allow_inf_nan: only applies to numbers, allows the field to be NaN or infinity (+inf or -inf),
        which is a valid Python float. Default True, set to False for compatibility with JSON.
    :param max_digits: only applies to Decimals, requires the field to have a maximum number
      of digits within the decimal. It does not include a zero before the decimal point or trailing decimal zeroes.
    :param decimal_places: only applies to Decimals, requires the field to have at most a number of decimal places
      allowed. It does not include trailing decimal zeroes.
    :param min_items: only applies to lists, requires the field to have a minimum number of
      elements. The schema will have a ``minItems`` validation keyword
    :param max_items: only applies to lists, requires the field to have a maximum number of
      elements. The schema will have a ``maxItems`` validation keyword
    :param unique_items: only applies to lists, requires the field not to have duplicated
      elements. The schema will have a ``uniqueItems`` validation keyword
    :param min_length: only applies to strings, requires the field to have a minimum length. The
      schema will have a ``minLength`` validation keyword
    :param max_length: only applies to strings, requires the field to have a maximum length. The
      schema will have a ``maxLength`` validation keyword
    :param allow_mutation: a boolean which defaults to True. When False, the field raises a TypeError if the field is
      assigned on an instance.  The BaseModel Config must set validate_assignment to True
    :param regex: only applies to strings, requires the field match against a regular expression
      pattern string. The schema will have a ``pattern`` validation keyword
    :param discriminator: only useful with a (discriminated a.k.a. tagged) `Union` of sub models with a common field.
      The `discriminator` is the name of this common field to shorten validation and improve generated schema
    :param repr: show this field in the representation
    :param ui_options: UIOptions instance used to set ui properties for the ui schema
    :param **extra: any additional keyword arguments will be added as is to the schema
    """
    field_info = FieldInfoWithUIOptions(
        default,
        default_factory=default_factory,
        alias=alias,
        title=title,
        description=description,
        exclude=exclude,
        include=include,
        const=const,
        gt=gt,
        ge=ge,
        lt=lt,
        le=le,
        multiple_of=multiple_of,
        allow_inf_nan=allow_inf_nan,
        max_digits=max_digits,
        decimal_places=decimal_places,
        min_items=min_items,
        max_items=max_items,
        unique_items=unique_items,
        min_length=min_length,
        max_length=max_length,
        allow_mutation=allow_mutation,
        regex=regex,
        discriminator=discriminator,
        repr=repr,
        ui_options=ui_options,
        **extra,
    )
    field_info._validate()
    return field_info


class UISchemaModelMixin:

    @classmethod
    def ui_schema(cls, *args, **kwargs):
        """Generates a UI schema from model"""
        ui_schema = {}
        # Iterate through the fields and generate UI schema
        for field_name, model_field in cls.__fields__.items():
            if getattr(model_field.field_info, "ui_options", None):
                ui_schema[field_name] = model_field.field_info.ui_schema()
        # Include global options
        if global_options := cls.__fields__.get('ui_global_options'):
            if getattr(global_options, "type_", None) == GlobalUISchemaOptions:
                model = global_options.default
                for field_name, model_field in model.__fields__.items():
                    if value := getattr(model, field_name, model_field.default):
                        ui_schema[f"ui:{field_name}"] = value
        return ui_schema


    @classmethod
    def schema(cls, **kwargs):
        # Call the parent schema method to get the original schema
        json_schema_dict = super().schema(**kwargs)

        # Remove ui schema fields from the properties and definitions
        properties = json_schema_dict.get('properties', {})
        for field in ["ui_options", "ui_global_options"]:
            properties.pop(field, None)
        json_schema_dict['properties'] = properties
        definitions = json_schema_dict.get('definitions', {})
        for field in ["UIOptions", "GlobalUISchemaOptions"]:
            definitions.pop(field, None)
        json_schema_dict['definitions'] = definitions
        return json_schema_dict
</file>

<file path="gundi-integration-tahmo-weather/app/settings/__init__.py">
from .base import *
from .integration import *
</file>

<file path="gundi-integration-tahmo-weather/app/settings/base.py">
import logging.config
import sys
from environs import Env

env = Env()
env.read_env()

LOGGING_LEVEL = env.str("LOGGING_LEVEL", "INFO")

DEFAULT_LOGGING = {
    "version": 1,
    "disable_existing_loggers": False,
    "handlers": {
        "console": {
            "level": LOGGING_LEVEL,
            "class": "logging.StreamHandler",
            "stream": sys.stdout
        },
    },
    "loggers": {
        "": {
            "handlers": ["console"],
            "level": LOGGING_LEVEL,
        },
    },
}
logging.config.dictConfig(DEFAULT_LOGGING)

DEFAULT_REQUESTS_TIMEOUT = (10, 20)  # Connect, Read

CDIP_API_ENDPOINT = env.str("CDIP_API_ENDPOINT", None)
CDIP_ADMIN_ENDPOINT = env.str("CDIP_ADMIN_ENDPOINT", None)
PORTAL_API_ENDPOINT = f"{CDIP_ADMIN_ENDPOINT}/api/v1.0"
PORTAL_OUTBOUND_INTEGRATIONS_ENDPOINT = (
    f"{PORTAL_API_ENDPOINT}/integrations/outbound/configurations"
)
PORTAL_INBOUND_INTEGRATIONS_ENDPOINT = (
    f"{PORTAL_API_ENDPOINT}/integrations/inbound/configurations"
)
GUNDI_API_BASE_URL = env.str("GUNDI_API_BASE_URL", None)
GUNDI_API_SSL_VERIFY = env.bool("GUNDI_API_SSL_VERIFY", True)
SENSORS_API_BASE_URL = env.str("SENSORS_API_BASE_URL", None)

# Used in OTel traces/spans to set the 'environment' attribute, used on metrics calculation
TRACE_ENVIRONMENT = env.str("TRACE_ENVIRONMENT", "dev")

# GCP related settings
GCP_PROJECT_ID = env.str("GCP_PROJECT_ID", "cdip-78ca")


KEYCLOAK_ALGORITHMS = env.list("KEYCLOAK_ALGORITHMS", ["RS256", "HS256"])
KEYCLOAK_AUDIENCE = env.str("KEYCLOAK_AUDIENCE", None)
KEYCLOAK_AUTH_SERVICE = env.str("KEYCLOAK_AUTH_SERVICE", None)
KEYCLOAK_REALM = env.str("KEYCLOAK_REALM", None)
KEYCLOAK_ISSUER = f"{KEYCLOAK_AUTH_SERVICE}/realms/{KEYCLOAK_REALM}"

# Redis settings for state manager
REDIS_HOST = env.str("REDIS_HOST", "localhost")
REDIS_PORT = env.int("REDIS_PORT", 6379)
REDIS_STATE_DB = env.int("REDIS_STATE_DB", 0)


# Settings for system events
INTEGRATION_EVENTS_TOPIC = env.str("INTEGRATION_EVENTS_TOPIC", "integration-events")

PROCESS_PUBSUB_MESSAGES_IN_BACKGROUND = env.bool("PROCESS_PUBSUB_MESSAGES_IN_BACKGROUND", True)
</file>

<file path="gundi-integration-tahmo-weather/app/settings/integration.py">
# Add your integration-specific settings here
</file>

<file path="gundi-integration-tahmo-weather/app/api_schemas.py">
from pydantic import BaseModel


class ActionRequest(BaseModel):
    integration_id: str
    action_id: str
    run_in_background: bool = False
</file>

<file path="gundi-integration-tahmo-weather/app/conftest.py">
import asyncio
import datetime
import json

import pytest
from unittest.mock import MagicMock
from app import settings
from gcloud.aio import pubsub
from gundi_core.schemas.v2 import Integration, IntegrationActionConfiguration, IntegrationActionSummary
from gundi_core.events import (
    SystemEventBaseModel,
    IntegrationActionCustomLog,
    CustomActivityLog,
    IntegrationActionStarted,
    ActionExecutionStarted,
    IntegrationActionFailed,
    ActionExecutionFailed,
    IntegrationActionComplete,
    ActionExecutionComplete,
    LogLevel
)


class AsyncMock(MagicMock):
    async def __call__(self, *args, **kwargs):
        return super(AsyncMock, self).__call__(*args, **kwargs)


def async_return(result):
    f = asyncio.Future()
    f.set_result(result)
    return f


@pytest.fixture
def mock_integration_state():
    return {"last_execution": "2024-01-29T11:20:00+0200"}


@pytest.fixture
def mock_redis(mocker, mock_integration_state):
    redis = MagicMock()
    redis_client = mocker.MagicMock()
    redis_client.set.return_value = async_return(MagicMock())
    redis_client.get.return_value = async_return(json.dumps(mock_integration_state, default=str))
    redis_client.delete.return_value = async_return(MagicMock())
    redis_client.setex.return_value = async_return(None)
    redis_client.incr.return_value = redis_client
    redis_client.decr.return_value = async_return(None)
    redis_client.expire.return_value = redis_client
    redis_client.execute.return_value = async_return((1, True))
    redis_client.__aenter__.return_value = redis_client
    redis_client.__aexit__.return_value = None
    redis_client.pipeline.return_value = redis_client
    redis.Redis.return_value = redis_client
    return redis


@pytest.fixture
def integration_v2():
    return Integration.parse_obj(
        {'id': '779ff3ab-5589-4f4c-9e0a-ae8d6c9edff0', 'name': 'Gundi X', 'base_url': 'https://gundi-er.pamdas.org',
         'enabled': True,
         'type': {'id': '50229e21-a9fe-4caa-862c-8592dfb2479b', 'name': 'EarthRanger', 'value': 'earth_ranger',
                  'description': 'Integration type for Integration X Sites', 'actions': [
                 {'id': '80448d1c-4696-4b32-a59f-f3494fc949ac', 'type': 'auth', 'name': 'Authenticate', 'value': 'auth',
                  'description': 'Authenticate against Integration X',
                  'schema': {'type': 'object', 'required': ['token'], 'properties': {'token': {'type': 'string'}}}},
                 {'id': '4b721b37-f4ca-4f20-b07c-2caadb095ecb', 'type': 'pull', 'name': 'Pull Events',
                  'value': 'pull_events', 'description': 'Extract events from EarthRanger sites',
                  'schema': {'type': 'object', 'title': 'PullObservationsConfig', 'required': ['start_datetime'],
                             'properties': {'start_datetime': {'type': 'string', 'title': 'Start Datetime'}}}},
                 {'id': '75b3040f-ab1f-42e7-b39f-8965c088b154', 'type': 'pull', 'name': 'Pull Observations',
                  'value': 'pull_observations', 'description': 'Extract observations from an EarthRanger Site',
                  'schema': {'type': 'object', 'title': 'PullObservationsConfig', 'required': ['start_datetime'],
                             'properties': {'start_datetime': {'type': 'string', 'title': 'Start Datetime'}}}},
                 {'id': '425a2e2f-ae71-44fb-9314-bc0116638e4f', 'type': 'push', 'name': 'Push Event Attachments',
                  'value': 'push_event_attachments',
                  'description': 'EarthRanger sites support adding attachments to events', 'schema': {}},
                 {'id': '8e101f31-e693-404c-b6ee-20fde6019f16', 'type': 'push', 'name': 'Push Events',
                  'value': 'push_events', 'description': 'EarthRanger sites support sending Events (a.k.a Reports)',
                  'schema': {}}]},
         'owner': {'id': 'a91b400b-482a-4546-8fcb-ee42b01deeb6', 'name': 'Test Org', 'description': ''},
         'configurations': [
             {'id': '5577c323-b961-4277-9047-b1f27fd6a1b7', 'integration': '779ff3ab-5589-4f4c-9e0a-ae8d6c9edff0',
              'action': {'id': '75b3040f-ab1f-42e7-b39f-8965c088b154', 'type': 'pull', 'name': 'Pull Observations',
                         'value': 'pull_observations'},
              'data': {'end_datetime': '2023-11-10T06:00:00-00:00', 'start_datetime': '2023-11-10T05:30:00-00:00',
                       'force_run_since_start': False}},
             {'id': '431af42b-c431-40af-8b57-a349253e15df', 'integration': '779ff3ab-5589-4f4c-9e0a-ae8d6c9edff0',
              'action': {'id': '4b721b37-f4ca-4f20-b07c-2caadb095ecb', 'type': 'pull', 'name': 'Pull Events',
                         'value': 'pull_events'}, 'data': {'start_datetime': '2023-11-16T00:00:00-03:00'}},
             {'id': '30f8878c-4a98-4c95-88eb-79f73c40fb2f', 'integration': '779ff3ab-5589-4f4c-9e0a-ae8d6c9edff0',
              'action': {'id': '80448d1c-4696-4b32-a59f-f3494fc949ac', 'type': 'auth', 'name': 'Authenticate',
                         'value': 'auth'}, 'data': {'token': 'testtoken2a97022f21732461ee103a08fac8a35'}}],
         'additional': {},
         'default_route': {'id': '5abf3845-7c9f-478a-bc0f-b24d87038c4b', 'name': 'Gundi X Provider - Default Route'},
         'status': 'healthy',
         'status_details': ''
        }
    )


@pytest.fixture
def pull_observations_config():
    return IntegrationActionConfiguration(
        id='b3cdc6b2-b247-4fbd-8f86-53079b5860e5',
        integration='779ff3ab-5589-4f4c-9e0a-ae8d6c9edff0',
        action=IntegrationActionSummary(
            id='2e52c0ba-1723-4510-8702-496c232b2012',
            type='pull',
            name='Pull Observations',
            value='pull_observations'
        ),
        data={}
    )


@pytest.fixture
def mock_gundi_client_v2(
        mocker,
        integration_v2,
        mock_get_gundi_api_key
):
    mock_client = mocker.MagicMock()
    mock_client.get_integration_api_key.return_value = async_return(mock_get_gundi_api_key),
    mock_client.get_integration_details.return_value = async_return(
        integration_v2
    )
    mock_client.__aenter__.return_value = mock_client
    return mock_client


@pytest.fixture
def mock_gundi_client_v2_class(mocker, mock_gundi_client_v2):
    mock_gundi_client_v2_class = mocker.MagicMock()
    mock_gundi_client_v2_class.return_value = mock_gundi_client_v2
    return mock_gundi_client_v2_class


@pytest.fixture
def mock_gundi_sensors_client_class(mocker, events_created_response, observations_created_response):
    mock_gundi_sensors_client_class = mocker.MagicMock()
    mock_gundi_sensors_client = mocker.MagicMock()
    mock_gundi_sensors_client.post_events.return_value = async_return(
        events_created_response
    )
    mock_gundi_sensors_client.post_observations.return_value = async_return(
        observations_created_response
    )
    mock_gundi_sensors_client_class.return_value = mock_gundi_sensors_client
    return mock_gundi_sensors_client_class


@pytest.fixture
def events_created_response():
    return [
        {
            "id": "e1e1e1e1-e1e1-e1e1-e1e1-e1e1e1e1e1e1",
            "title": "Animal Sighting",
            "event_type": "wildlife_sighting_rep",
            "recorded_at": "2024-01-29 20:51:10-03:00",
            "location": {
                "lat": -51.688645,
                "lon": -72.704421
            },
            "event_details": {
                "site_name": "MM Spot",
                "species": "lion"
            }
        },
        {
            "id": "e1e1e1e1-e1e1-e1e1-e1e1-e1e1e1e1e1e2",
            "title": "Animal Sighting",
            "event_type": "wildlife_sighting_rep",
            "recorded_at": "2024-01-29 20:51:25-03:00",
            "location": {
                "lat": -51.688646,
                "lon": -72.704421
            },
            "event_details": {
                "site_name": "MM Spot",
                "species": "lion"
            }
        }
    ]


@pytest.fixture
def observations_created_response():
    return [
        {
            "id": "e1e1e1e1-e1e1-e1e1-e1e1-e1e1e1e1e1e1",
            "source": "device-xy123",
            "type": "tracking-device",
            "subject_type": "puma",
            "recorded_at": "2024-01-24 09:03:00-0300",
            "location": {
                "lat": -51.748,
                "lon": -72.720
            },
            "additional": {
                "speed_kmph": 5
            }
        },
        {
            "id": "e1e1e1e1-e1e1-e1e1-e1e1-e1e1e1e1e1e2",
            "source": "test-device-mariano",
            "type": "tracking-device",
            "subject_type": "puma",
            "recorded_at": "2024-01-24 09:05:00-0300",
            "location": {
                "lat": -51.755,
                "lon": -72.755
            },
            "additional": {
                "speed_kmph": 5
            }
        }
    ]


@pytest.fixture
def mock_state_manager(mocker):
    mock_state_manager = mocker.MagicMock()
    mock_state_manager.get_state.return_value = async_return(
        {'last_execution': '2023-11-17T11:20:00+0200'}
    )
    mock_state_manager.set_state.return_value = async_return(None)
    return mock_state_manager


@pytest.fixture
def mock_pubsub_client(
        mocker, integration_event_pubsub_message, gcp_pubsub_publish_response
):
    mock_client = mocker.MagicMock()
    mock_publisher = mocker.MagicMock()
    mock_publisher.publish.return_value = async_return(gcp_pubsub_publish_response)
    mock_publisher.topic_path.return_value = (
        f"projects/{settings.GCP_PROJECT_ID}/topics/{settings.INTEGRATION_EVENTS_TOPIC}"
    )
    mock_client.PublisherClient.return_value = mock_publisher
    mock_client.PubsubMessage.return_value = integration_event_pubsub_message
    return mock_client


@pytest.fixture
def integration_event_pubsub_message():
    return pubsub.PubsubMessage(
        b'{"event_id": "6214c049-f786-45eb-9877-2efb2c2cf8e9", "timestamp": "2024-01-26 14:03:46.199385+00:00", "schema_version": "v1", "payload": {"integration_id": "779ff3ab-5589-4f4c-9e0a-ae8d6c9edff0", "action_id": "pull_observations", "config_data": {"end_datetime": "2024-01-01T00:00:00-00:00", "start_datetime": "2024-01-10T23:59:59-00:00", "force_run_since_start": true}}, "event_type": "IntegrationActionStarted"}'
    )


@pytest.fixture
def gcp_pubsub_publish_response():
    return {"messageIds": ["7061707768812258"]}


@pytest.fixture
def mock_publish_event(gcp_pubsub_publish_response):
    mock_publish_event = AsyncMock()
    mock_publish_event.return_value = gcp_pubsub_publish_response
    return mock_publish_event


@pytest.fixture
def mock_action_handlers(mocker):
    mock_action_handler = AsyncMock()
    mock_action_handler.return_value = {"observations_extracted": 10}
    mock_action_handlers = mocker.MagicMock()
    mock_action_handlers.__getitem__.return_value = mock_action_handler
    return mock_action_handlers


@pytest.fixture
def auth_headers_response():
    return {
        'Accept-Type': 'application/json',
        'Authorization': 'Bearer testtoken2a97022f21732461ee103a08fac8a35'
    }


@pytest.fixture
def mock_get_gundi_api_key(mocker, mock_api_key):
    mock = mocker.MagicMock()
    mock.return_value = async_return(mock_api_key)
    return mock


@pytest.fixture
def mock_api_key():
    return "MockAP1K3y"


@pytest.fixture
def event_v2_cloud_event_payload():
    timestamp = datetime.datetime.now().strftime("%Y-%m-%dT%H:%M:%S.%fZ")
    return {
        "message": {
            "data": "eyJpbnRlZ3JhdGlvbl9pZCI6ICI4NDNlMDgwMS1lODFhLTQ3ZTUtOWNlMi1iMTc2ZTQ3MzZhODUiLCAiYWN0aW9uX2lkIjogInB1bGxfb2JzZXJ2YXRpb25zIn0=",
            "messageId": "10298788169291041", "message_id": "10298788169291041",
            "publishTime": timestamp,
            "publish_time": timestamp
        },
        "subscription": "projects/cdip-stage-78ca/subscriptions/integrationx-actions-sub"
    }


@pytest.fixture
def event_v2_cloud_event_headers():
    timestamp = datetime.datetime.now().strftime("%Y-%m-%dT%H:%M:%S.%fZ")
    return {
        "host": "integrationx-actions-runner-jabcutl7za-uc.a.run.app",
        "content-type": "application/json",
        "authorization": "Bearer fake-token",
        "content-length": "2057",
        "accept": "application/json",
        "from": "noreply@google.com",
        "user-agent": "APIs-Google; (+https://developers.google.com/webmasters/APIs-Google.html)",
        "x-cloud-trace-context": "",
        "traceparent": "",
        "x-forwarded-for": "64.233.172.137",
        "x-forwarded-proto": "https",
        "forwarded": 'for="64.233.172.137";proto=https',
        "accept-encoding": "gzip, deflate, br",
        "ce-id": "20090163454824831",
        "ce-source": "//pubsub.googleapis.com/projects/cdip-stage-78ca/topics/integrationx-actions-topic-test",
        "ce-specversion": "1.0",
        "ce-type": "google.cloud.pubsub.topic.v1.messagePublished",
        "ce-time": timestamp,
    }


@pytest.fixture
def action_started_event():
    return IntegrationActionStarted(
        payload=ActionExecutionStarted(
            integration_id="779ff3ab-5589-4f4c-9e0a-ae8d6c9edff0",
            action_id="pull_observations",
            config_data={
                "end_datetime": "2024-01-10T00:00:00-00:00",
                "start_datetime": "2024-01-01T23:59:59-00:00",
                "force_run_since_start": True
            },
        )
    )


@pytest.fixture
def action_complete_event():
    return IntegrationActionComplete(
        payload=ActionExecutionComplete(
            integration_id="779ff3ab-5589-4f4c-9e0a-ae8d6c9edff0",
            action_id="pull_observations",
            config_data={
                "end_datetime": "2024-01-10T00:00:00-00:00",
                "start_datetime": "2024-01-01T23:59:59-00:00",
                "force_run_since_start": True
            },
            result={"observations_extracted": 10}
        )
    )

@pytest.fixture
def action_failed_event():
    return IntegrationActionFailed(
        payload=ActionExecutionFailed(
            integration_id="779ff3ab-5589-4f4c-9e0a-ae8d6c9edff0",
            action_id="pull_observations",
            config_data={
                "end_datetime": "2024-01-10T00:00:00-00:00",
                "start_datetime": "2024-01-01T23:59:59-00:00",
                "force_run_since_start": True
            },
            error="ConnectionError: Error connecting to X system"
        )
    )


@pytest.fixture
def custom_activity_log_event():
    return IntegrationActionCustomLog(
        payload=CustomActivityLog(
            integration_id="779ff3ab-5589-4f4c-9e0a-ae8d6c9edff0",
            action_id="pull_observations",
            config_data={
                "end_datetime": "2024-01-01T00:00:00-00:00",
                "start_datetime": "2024-01-10T23:59:59-00:00",
                "force_run_since_start": True
            },
            title="Invalid start_datetime for action pull_observations",
            level=LogLevel.ERROR,
            data={
                "details": "start_datetime cannot be grater than end_datetime. Please fix the configuration."
            }
        )
    )


@pytest.fixture
def system_event(request, action_started_event, action_complete_event, action_failed_event, custom_activity_log_event):
    if request.param == "action_started_event":
        return action_started_event
    if request.param == "action_complete_event":
        return action_complete_event
    if request.param == "action_failed_event":
        return action_failed_event
    if request.param == "custom_activity_log_event":
        return custom_activity_log_event
    return None
</file>

<file path="gundi-integration-tahmo-weather/app/main.py">
import base64
import json
import logging
import os
import uvicorn
from fastapi import FastAPI, Request, status, Depends, BackgroundTasks
from fastapi.encoders import jsonable_encoder
from fastapi.exceptions import RequestValidationError
from fastapi.responses import JSONResponse
from app.routers import (
    actions,
)
import app.settings as settings
from fastapi.middleware.cors import CORSMiddleware

from app.services.action_runner import execute_action

# For running behind a proxy, we'll want to configure the root path for OpenAPI browser.
root_path = os.environ.get("ROOT_PATH", "")
app = FastAPI(
    title="Gundi Integration Actions Execution Service",
    description="API to trigger actions against third-party systems",
    version="1",
)

origins = [
    "*",
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

logger = logging.getLogger(__name__)


@app.get(
    "/",
    tags=["health-check"],
    summary="Check that the service is healthy",
    description="This is primarily used to test authentication. It allows a caller to see whether it has successfully authenticated or is identified as _anonymous_.",
)
def read_root(
    request: Request,
):
    return {"status": "healthy"}


@app.post(
    "/",
    summary="Execute an action from GCP PubSub",
)
async def execute(
    request: Request,
    background_tasks: BackgroundTasks
):
    body = await request.body()
    print(f"Message Received. RAW body: {body}")
    json_data = await request.json()
    print(f"JSON: {json_data}")
    payload = base64.b64decode(json_data["message"]["data"]).decode("utf-8").strip()
    print(f"Payload: {payload}")
    json_payload = json.loads(payload)
    print(f"JSON Payload: {json_payload}")
    if settings.PROCESS_PUBSUB_MESSAGES_IN_BACKGROUND:
        background_tasks.add_task(
            execute_action,
            integration_id=json_payload.get("integration_id"),
            action_id=json_payload.get("action_id"),
            config_overrides=json_payload.get("config_overrides"),
        )
    else:
        await execute_action(
            integration_id=json_payload.get("integration_id"),
            action_id=json_payload.get("action_id"),
            config_overrides=json_payload.get("config_overrides"),
        )
    return {}


app.include_router(
    actions.router, prefix="/v1/actions", tags=["actions"], responses={}
)


@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):

    logger.debug(
        "Failed handling body: %s",
        jsonable_encoder({"detail": exc.errors(), "body": exc.body}),
    )

    return JSONResponse(
        status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
        content=jsonable_encoder({"detail": exc.errors(), "body": exc.body}),
    )
</file>

<file path="gundi-integration-tahmo-weather/docker/docker-compose.yml">
version: "3.9"

services:
  api:
    container_name: api
    build:
      context: ../
      dockerfile: ./Dockerfile
    ports:
      - 8080:8080
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://0.0.0.0:8080/"]
      interval: 15s
      timeout: 15s
      retries: 10
</file>

<file path="gundi-integration-tahmo-weather/docker/Dockerfile">
FROM python:3.10-slim

# This command is for webhooks support
RUN apt-get update && apt-get install -y autoconf automake libtool make python3-dev

WORKDIR /code

COPY requirements.txt .
RUN pip install --upgrade pip && pip install -r /code/requirements.txt
COPY ./app app/

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8080"]
</file>

<file path="gundi-integration-tahmo-weather/.gitignore">
# Additional
.idea/

# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
pip-wheel-metadata/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
.python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# PEP 582; used by e.g. github.com/David-OConnor/pyflow
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

.DS_Store
.idea
.vscode
</file>

<file path="gundi-integration-tahmo-weather/LICENSE">
Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
</file>

<file path="gundi-integration-tahmo-weather/README.md">
# gundi-integration-action-runner
Template repo for integration in Gundi v2.

## Usage
- Fork this repo
- Implement your own actions in `actions/handlers.py`
- Define configurations needed for your actions in `action/config.py`
- Optionally, add the @activity_logger decorator to log common events which you can later see in the portal:
    - Action execution started
    - Action execution finished
    - Error occurred during action execution
- Optionally, use the `log_activity()` method to log custom messages which you can later see in the portal


Example: `actions/handlers.py`
```python
from app.services.activity_logger import activity_logger, log_activity
from app.services.gundi import send_observations_to_gundi
from gundi_core.events import LogLevel


@activity_logger()
async def action_pull_observations(integration, action_config):
    
    # Add your business logic to extract data here...
    
    # Optionally, log a custom messages to be shown in the portal
    await log_activity(
        integration_id=integration.id,
        action_id="pull_observations",
        level=LogLevel.INFO,
        title="Extracting observations with filter..",
        data={"start_date": "2024-01-01", "end_date": "2024-01-31"},
        config_data=action_config.data
    )
    
    # Normalize the extracted data into a list of observations following to the Gundi schema:
    observations = [
        {
            "source": "collar-xy123",
            "type": "tracking-device",
            "subject_type": "puma",
            "recorded_at": "2024-01-24 09:03:00-0300",
            "location": {
                "lat": -51.748,
                "lon": -72.720
            },
            "additional": {
                "speed_kmph": 10
            }
        }
    ]
    
    # Send the extracted data to Gundi
    await send_observations_to_gundi(observations=observations, integration_id=integration.id)

    # The result will be recorded in the portal if using the activity_logger decorator
    return {"events_extracted": 10}
```
</file>

<file path="gundi-integration-tahmo-weather/requirements-base.in">
# Base dependencies. Don't edit or override this file.
# Use requirements.in to add integration-specific dependencies instead.
environs~=9.5.0
pydantic~=1.10.13
fastapi~=0.103.2
uvicorn~=0.23.2
gundi-core~=1.7.1
gundi-client-v2~=2.3.8
stamina~=23.2.0
redis~=5.0.1
gcloud-aio-pubsub~=6.0.0
</file>

<file path="gundi-integration-tahmo-weather/requirements-dev.in">
# Dependencies for development. Don't edit or override this file.
# Use requirements.in to add integration-specific dependencies instead.
pytest~=7.4.3
pytest-asyncio~=0.21.1
pytest-mock~=3.12.0
</file>

<file path="gundi-integration-tahmo-weather/requirements.in">
# Add your integration-specific dependencies here
</file>

<file path="gundi-integration-tahmo-weather/requirements.txt">
#
# This file is autogenerated by pip-compile with Python 3.10
# by the following command:
#
#    pip-compile --output-file=requirements.txt requirements-base.in requirements-dev.in requirements.in
#
aiohttp==3.9.3
    # via gcloud-aio-auth
aiosignal==1.3.1
    # via aiohttp
anyio==3.7.1
    # via
    #   fastapi
    #   httpcore
    #   starlette
async-timeout==4.0.3
    # via
    #   aiohttp
    #   redis
attrs==23.2.0
    # via aiohttp
backoff==2.2.1
    # via gcloud-aio-auth
certifi==2024.2.2
    # via
    #   httpcore
    #   httpx
cffi==1.16.0
    # via cryptography
chardet==5.2.0
    # via gcloud-aio-auth
click==8.1.7
    # via uvicorn
cryptography==42.0.5
    # via gcloud-aio-auth
environs==9.5.0
    # via
    #   -r requirements-base.in
    #   gundi-client-v2
exceptiongroup==1.2.2
    # via
    #   anyio
    #   pytest
fastapi==0.103.2
    # via -r requirements-base.in
frozenlist==1.4.1
    # via
    #   aiohttp
    #   aiosignal
gcloud-aio-auth==5.2.2
    # via gcloud-aio-pubsub
gcloud-aio-pubsub==6.0.1
    # via -r requirements-base.in
gundi-client-v2==2.3.8
    # via -r requirements-base.in
gundi-core==1.7.1
    # via
    #   -r requirements-base.in
    #   gundi-client-v2
h11==0.14.0
    # via
    #   httpcore
    #   uvicorn
httpcore==0.17.3
    # via httpx
httpx==0.24.1
    # via
    #   gundi-client-v2
    #   respx
idna==3.6
    # via
    #   anyio
    #   httpx
    #   yarl
iniconfig==2.0.0
    # via pytest
marshmallow==3.21.1
    # via environs
multidict==6.0.5
    # via
    #   aiohttp
    #   yarl
packaging==24.0
    # via
    #   marshmallow
    #   pytest
pluggy==1.4.0
    # via pytest
prometheus-client==0.20.0
    # via gcloud-aio-pubsub
pycparser==2.21
    # via cffi
pydantic==1.10.14
    # via
    #   -r requirements-base.in
    #   fastapi
    #   gundi-client-v2
    #   gundi-core
pyjwt==2.8.0
    # via gcloud-aio-auth
pytest==7.4.4
    # via
    #   -r requirements-dev.in
    #   pytest-asyncio
    #   pytest-mock
pytest-asyncio==0.21.1
    # via -r requirements-dev.in
pytest-mock==3.12.0
    # via -r requirements-dev.in
python-dotenv==1.0.1
    # via environs
redis==5.0.3
    # via -r requirements-base.in
respx==0.20.2
    # via gundi-client-v2
sniffio==1.3.1
    # via
    #   anyio
    #   httpcore
    #   httpx
stamina==23.2.0
    # via -r requirements-base.in
starlette==0.27.0
    # via fastapi
tenacity==8.2.3
    # via stamina
tomli==2.0.2
    # via pytest
typing-extensions==4.10.0
    # via
    #   fastapi
    #   pydantic
    #   uvicorn
uvicorn==0.23.2
    # via -r requirements-base.in
yarl==1.9.4
    # via aiohttp
</file>

</files>
